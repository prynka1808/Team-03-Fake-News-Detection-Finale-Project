{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import os\n",
        "import zipfile\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "czrtgGU-8vBb"
      },
      "id": "czrtgGU-8vBb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW6m79_O8xcR",
        "outputId": "b967018e-bcde-4ad9-98be-fd5ae812118b"
      },
      "id": "kW6m79_O8xcR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    import subprocess\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "zlVLlD4P8y-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b03fddf-e7fd-49ae-886b-ce7d50b295e3"
      },
      "id": "zlVLlD4P8y-N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize necessary tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "C-C9zdFj81Hk"
      },
      "id": "C-C9zdFj81Hk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean raw text data\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "        text = re.sub(r'\\[.*?\\]', '', text)  # Remove text inside brackets\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "        return text.strip()\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "0r6rDcIP83WU"
      },
      "id": "0r6rDcIP83WU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for tokenization, stopword removal, and lemmatization\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "xrrAnyCc85e8"
      },
      "id": "xrrAnyCc85e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for sentiment analysis\n",
        "def sentiment_score(text):\n",
        "    return TextBlob(text).sentiment.polarity"
      ],
      "metadata": {
        "id": "wg6_pkEo87ox"
      },
      "id": "wg6_pkEo87ox",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for word embedding using Word2Vec\n",
        "def train_word2vec(corpus):\n",
        "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
        "    model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model"
      ],
      "metadata": {
        "id": "4QgNYYiz89Ln"
      },
      "id": "4QgNYYiz89Ln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub raw file URL\n",
        "liar_url = \"https://raw.githubusercontent.com/prynka1808/Team-03-Fake-News-Detection-Finale-Project/refs/heads/main/Dataset/liar-fake-news-dataset/train.tsv\"\n",
        "\n",
        "try:\n",
        "    liar_df = pd.read_csv(liar_url, delimiter=\"\\t\", names=[\"label\", \"text\"])\n",
        "    liar_df['label'] = liar_df['label'].map({'true': 0, 'false': 1})  # Real: 0, Fake: 1\n",
        "    liar_df[\"id\"] = \"No ID\"  # LIAR dataset has no ID\n",
        "    liar_df[\"news_url\"] = \"No URL\"  # LIAR dataset has no URL\n",
        "    print(\"LIAR dataset loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LIAR dataset: {e}\")\n",
        "    liar_df = pd.DataFrame(columns=[\"id\", \"news_url\", \"text\", \"label\"])  # Create empty DataFrame if loading fails\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28GIxBxlz5Qn",
        "outputId": "1b64410e-194e-4ba0-f35e-baeb02677586"
      },
      "id": "28GIxBxlz5Qn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIAR dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub raw URLs\n",
        "fake_news_net_url = \"https://raw.githubusercontent.com/prynka1808/Team-03-Fake-News-Detection-Finale-Project/main/Dataset/FakeNewsNet-github/dataset/politifact_fake.csv\"\n",
        "real_news_net_url = \"https://raw.githubusercontent.com/prynka1808/Team-03-Fake-News-Detection-Finale-Project/main/Dataset/FakeNewsNet-github/dataset/politifact_real.csv\"\n",
        "\n",
        "try:\n",
        "    fake_news_net_df = pd.read_csv(fake_news_net_url)\n",
        "    real_news_net_df = pd.read_csv(real_news_net_url)\n",
        "\n",
        "    fake_news_net_df['label'] = 1  # Fake news\n",
        "    real_news_net_df['label'] = 0  # Real news\n",
        "\n",
        "    # Rename 'title' to 'text' for consistency\n",
        "    fake_news_net_df.rename(columns={\"title\": \"text\"}, inplace=True)\n",
        "    real_news_net_df.rename(columns={\"title\": \"text\"}, inplace=True)\n",
        "\n",
        "    fakenewsnet_df = pd.concat([fake_news_net_df, real_news_net_df], ignore_index=True)\n",
        "    print(\"FakeNewsNet dataset loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FakeNewsNet dataset: {e}\")\n",
        "    fakenewsnet_df = pd.DataFrame(columns=[\"id\", \"news_url\", \"text\", \"label\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM_P4e71BCN0",
        "outputId": "a01cc6f8-66d8-44ab-987b-000193e76666"
      },
      "id": "oM_P4e71BCN0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FakeNewsNet dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# GitHub raw file URLs\n",
        "fake_articles_url = \"https://raw.github.com/prynka1808/Team-03-Fake-News-Detection-Finale-Project/main/Dataset/Fake-News-Dataset-kaggle/Testing_dataset/testingSet/Catalog%20-%20Fake%20Articles.csv\"\n",
        "real_articles_url = \"https://raw.github.com/prynka1808/Team-03-Fake-News-Detection-Finale-Project/main/Dataset/Fake-News-Dataset-kaggle/Testing_dataset/testingSet/Catalog%20-%20Real%20Articles.csv\"\n",
        "\n",
        "try:\n",
        "    fake_news_kaggle = pd.read_csv(fake_articles_url, encoding=\"utf-8\")\n",
        "    real_news_kaggle = pd.read_csv(real_articles_url, encoding=\"utf-8\")\n",
        "\n",
        "    fake_news_kaggle = fake_news_kaggle[['Article']].rename(columns={'Article': 'text'})\n",
        "    real_news_kaggle = real_news_kaggle[['Article']].rename(columns={'Article': 'text'})\n",
        "\n",
        "    fake_news_kaggle['label'] = 1  # Fake news\n",
        "    real_news_kaggle['label'] = 0  # Real news\n",
        "\n",
        "    fakenews_kaggle_df = pd.concat([fake_news_kaggle, real_news_kaggle], ignore_index=True)\n",
        "    fakenews_kaggle_df[\"id\"] = \"No ID\"\n",
        "    fakenews_kaggle_df[\"news_url\"] = \"No URL\"\n",
        "    print(\"Kaggle Fake News dataset loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Kaggle Fake News dataset: {e}\")\n",
        "    fakenews_kaggle_df = pd.DataFrame(columns=[\"id\", \"news_url\", \"text\", \"label\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqrdOCcF9Eqb",
        "outputId": "a2fffae9-d042-4ae0-9a69-4b0aec35729f"
      },
      "id": "JqrdOCcF9Eqb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle Fake News dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all datasets\n",
        "all_data = pd.concat([\n",
        "    liar_df[['id', 'news_url', 'text', 'label']],\n",
        "    fakenewsnet_df[['id', 'news_url', 'text', 'label']],\n",
        "    fakenews_kaggle_df[['id', 'news_url', 'text', 'label']]\n",
        "], ignore_index=True)"
      ],
      "metadata": {
        "id": "LAa3r5OK9Ho8"
      },
      "id": "LAa3r5OK9Ho8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text processing\n",
        "all_data['clean_text'] = all_data['text'].apply(clean_text)\n",
        "all_data['processed_text'] = all_data['clean_text'].apply(preprocess_text)\n",
        "all_data['sentiment'] = all_data['processed_text'].apply(sentiment_score)"
      ],
      "metadata": {
        "id": "B-KqBJywys6u"
      },
      "id": "B-KqBJywys6u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TF-IDF Feature Engineering\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_data['processed_text'])"
      ],
      "metadata": {
        "id": "5p5xDXnPyyOr"
      },
      "id": "5p5xDXnPyyOr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Word2Vec Embeddings\n",
        "word2vec_model = train_word2vec(all_data['processed_text'])"
      ],
      "metadata": {
        "id": "52MZIdNGyy9r"
      },
      "id": "52MZIdNGyy9r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text into numerical vectors\n",
        "def get_avg_word2vec(text, model):\n",
        "    words = text.split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0, dtype=np.float32) if vectors else np.zeros(100, dtype=np.float32)\n",
        "\n",
        "all_data['word2vec'] = all_data['processed_text'].apply(lambda x: get_avg_word2vec(x, word2vec_model))\n"
      ],
      "metadata": {
        "id": "2TWih51jy315"
      },
      "id": "2TWih51jy315",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Processed Data\n",
        "all_data.to_csv(\"processed_fake_news.csv\", index=False)\n",
        "import pickle\n",
        "with open(\"processed_fake_news.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_data, f)\n",
        "\n",
        "print(\"Processed dataset saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUR2kzhFy9IK",
        "outputId": "e6c2ae36-76e0-4b26-f11c-b9a6c950b10a"
      },
      "id": "GUR2kzhFy9IK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed dataset saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processed dataset\n",
        "df = pd.read_csv(\"processed_fake_news.csv\")\n",
        "\n",
        "# Verify the first few rows\n",
        "print(\"\\nSample Data\\n\")\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmtq_v1i7-ZV",
        "outputId": "b89dabc3-ed5b-4d2f-9efb-14c02ebb4dc1"
      },
      "id": "zmtq_v1i7-ZV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Data\n",
            "\n",
            "      id news_url                                      text  label  \\\n",
            "0  No ID   No URL                                  a mailer    NaN   \n",
            "1  No ID   No URL                           a floor speech.    NaN   \n",
            "2  No ID   No URL                                    Denver    NaN   \n",
            "3  No ID   No URL                            a news release    NaN   \n",
            "4  No ID   No URL                       an interview on CNN    NaN   \n",
            "5  No ID   No URL                 a an online opinion-piece    NaN   \n",
            "6  No ID   No URL                          a press release.    NaN   \n",
            "7  No ID   No URL  a Democratic debate in Philadelphia, Pa.    NaN   \n",
            "8  No ID   No URL                                a website     NaN   \n",
            "9  No ID   No URL                           an online video    NaN   \n",
            "\n",
            "                               clean_text                     processed_text  \\\n",
            "0                                a mailer                             mailer   \n",
            "1                          a floor speech                       floor speech   \n",
            "2                                  denver                             denver   \n",
            "3                          a news release                       news release   \n",
            "4                     an interview on cnn                      interview cnn   \n",
            "5                a an online opinionpiece                online opinionpiece   \n",
            "6                         a press release                      press release   \n",
            "7  a democratic debate in philadelphia pa  democratic debate philadelphia pa   \n",
            "8                               a website                            website   \n",
            "9                         an online video                       online video   \n",
            "\n",
            "   sentiment                                           word2vec  \n",
            "0        0.0  [-0.02195852  0.0049661   0.01044586 -0.011561...  \n",
            "1        0.0  [-2.02130422e-01  1.28115341e-03 -8.40963423e-...  \n",
            "2        0.0  [-9.60952044e-03 -2.98739731e-04 -5.48584620e-...  \n",
            "3        0.0  [ 2.69810520e-02  7.16786757e-02  3.44935711e-...  \n",
            "4        0.0  [-0.06900233  0.0962593   0.00614177 -0.017214...  \n",
            "5        0.0  [-3.87638658e-02  1.24037890e-02  6.47707283e-...  \n",
            "6        0.0  [ 0.01870485  0.06117135 -0.00413947  0.016903...  \n",
            "7        0.0  [-0.12560973  0.00816649 -0.03997732 -0.093579...  \n",
            "8        0.0  [-0.10363091  0.02180237  0.03266688 -0.049192...  \n",
            "9        0.0  [-9.79405940e-02  4.06534784e-02  1.98645033e-...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset label distribution\n",
        "print(\"\\nLabel Distribution by Source\")\n",
        "\n",
        "# LIAR Dataset Label Count\n",
        "print(\"\\nLIAR Dataset Labels:\\n\", df[df['news_url'] == \"No URL\"]['label'].value_counts())\n",
        "\n",
        "# FakeNewsNet Dataset Label Count\n",
        "print(\"\\nFakeNewsNet Dataset Labels:\\n\", df[df['news_url'] != \"No URL\"]['label'].value_counts())\n",
        "\n",
        "# FakeNewsKaggle Dataset Label Count\n",
        "print(\"\\nFakeNewsKaggle Dataset Labels:\\n\", df[df['id'] == \"No ID\"]['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0cTPNdH8QU-",
        "outputId": "1029ce6f-1469-4081-da7d-751340858c03"
      },
      "id": "-0cTPNdH8QU-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Distribution by Source\n",
            "\n",
            "LIAR Dataset Labels:\n",
            " label\n",
            "1.0    50\n",
            "0.0    50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "FakeNewsNet Dataset Labels:\n",
            " label\n",
            "0.0    624\n",
            "1.0    432\n",
            "Name: count, dtype: int64\n",
            "\n",
            "FakeNewsKaggle Dataset Labels:\n",
            " label\n",
            "1.0    50\n",
            "0.0    50\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tE8XDrfC-DPZ"
      },
      "id": "tE8XDrfC-DPZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}