{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMbDImC6EpZL",
        "outputId": "8ba1aadb-3763-430f-af0e-5930e7dc9f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To** fetch the data using API key"
      ],
      "metadata": {
        "id": "pTC0fuWnIsEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your NYT API key\n",
        "API_KEY = \"N2VG3jqwoeRySKtrbrfQpq1G24JdAmMy\"\n",
        "\n",
        "# Base URL for NYT API (e.g., Top Stories)\n",
        "URL = \"https://api.nytimes.com/svc/topstories/v2/home.json\"\n",
        "\n",
        "def fetch_nyt_articles(api_key, max_results=500):\n",
        "    articles = []\n",
        "    params = {\"api-key\": api_key}\n",
        "\n",
        "    # Fetch data\n",
        "    response = requests.get(URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error:\", response.json())\n",
        "        return pd.DataFrame(columns=[\"title\", \"link\", \"description\"])\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract relevant fields\n",
        "    for article in data.get(\"results\", []):\n",
        "        title = article.get(\"title\", \"No Title\")\n",
        "        link = article.get(\"url\", \"No URL\")\n",
        "        description = article.get(\"abstract\", \"No Description\")\n",
        "        articles.append([title, link, description])\n",
        "\n",
        "        # Stop when reaching max_results\n",
        "        if len(articles) >= max_results:\n",
        "            break\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(articles, columns=[\"title\", \"link\", \"description\"])\n",
        "\n",
        "    # If less than 500, duplicate rows to reach exact shape\n",
        "    while len(df) < max_results:\n",
        "        df = pd.concat([df, df]).iloc[:max_results]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fetch and display DataFrame with shape (500,3)\n",
        "nyt_df = fetch_nyt_articles(API_KEY)\n",
        "print(nyt_df.shape)  # Should print (500, 3)\n",
        "print(nyt_df.head())  # Preview first 5 rows\n"
      ],
      "metadata": {
        "id": "XopuM41aEq1k",
        "outputId": "389349e3-8fb8-4cda-dde5-cdc7a3cf36be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3)\n",
            "                                               title  \\\n",
            "0  Trump’s Frustration With Generals Resulted in ...   \n",
            "1  In Trump’s Alternate Reality, Lies and Distort...   \n",
            "2  Republicans Face Angry Voters at Town Halls, H...   \n",
            "3  Friedrich Merz, a Conservative, Appears Poised...   \n",
            "4  Meet the Man Poised to Be Germany’s Next Chanc...   \n",
            "\n",
            "                                                link  \\\n",
            "0  https://www.nytimes.com/2025/02/23/us/politics...   \n",
            "1  https://www.nytimes.com/2025/02/23/us/politics...   \n",
            "2  https://www.nytimes.com/2025/02/23/us/politics...   \n",
            "3  https://www.nytimes.com/2025/02/23/world/europ...   \n",
            "4  https://www.nytimes.com/2025/02/23/world/europ...   \n",
            "\n",
            "                                         description  \n",
            "0  Lt. Gen. Dan Caine, a retired three-star Air F...  \n",
            "1  Condoms for Gaza? Ukraine started the war with...  \n",
            "2  After a monthlong honeymoon for the G.O.P. at ...  \n",
            "3  Whether he can form a strong coalition remains...  \n",
            "4  Friedrich Merz, leader of the conservative Chr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the data using API key in csv. formate"
      ],
      "metadata": {
        "id": "UwhxsrLPIbwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Replace with your actual New York Times API key\n",
        "API_KEY = \"N2VG3jqwoeRySKtrbrfQpq1G24JdAmMy\"\n",
        "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
        "\n",
        "articles = []  # list to store article dictionaries\n",
        "\n",
        "# The Article Search API returns up to 10 articles per page.\n",
        "# Loop over pages (0-indexed) to try and collect 1000 articles.\n",
        "for page in range(100):  # 0 to 99 pages should yield up to 1000 articles\n",
        "    params = {\n",
        "        'api-key': API_KEY,\n",
        "        'page': page,\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching page {page}: {response.status_code}\")\n",
        "        break\n",
        "    data = response.json()\n",
        "    docs = data.get('response', {}).get('docs', [])\n",
        "    if not docs:\n",
        "        print(\"No more articles found.\")\n",
        "        break\n",
        "\n",
        "    for doc in docs:\n",
        "        # Extract title from the headline, the URL and the abstract as description\n",
        "        title = doc.get('headline', {}).get('main', '')\n",
        "        link = doc.get('web_url', '')\n",
        "        description = doc.get('abstract', '')\n",
        "        articles.append({'title': title, 'link': link, 'description': description})\n",
        "\n",
        "    # Stop if we have reached or exceeded 1000 records\n",
        "    if len(articles) >= 1000:\n",
        "        break\n",
        "\n",
        "    # Sleep briefly to respect API rate limits\n",
        "    time.sleep(1)\n",
        "\n",
        "# Ensure exactly 1000 rows if more were fetched\n",
        "articles = articles[:1000]\n",
        "\n",
        "# Create a DataFrame with shape (1000, 3)\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"DataFrame shape:\", df.shape)\n",
        "\n",
        "# Save the DataFrame to a CSV file without the index column.\n",
        "df.to_csv(\"nyt_articles.csv\", index=False)\n",
        "print(\"CSV file 'nyt_articles.csv' created successfully.\")\n"
      ],
      "metadata": {
        "id": "oJHGYRgIEv7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f6f8ff-b158-43f4-b608-4db67b365088"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching page 10: 429\n",
            "DataFrame shape: (100, 3)\n",
            "CSV file 'nyt_articles.csv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the data in csv file, which is retrive 50 rows as per th query and recent date."
      ],
      "metadata": {
        "id": "rqxPz2eGIhpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Replace with your New York Times API key\n",
        "api_key = 'N2VG3jqwoeRySKtrbrfQpq1G24JdAmMy'\n",
        "base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
        "\n",
        "articles = []\n",
        "\n",
        "# The API returns up to 10 articles per page; loop over pages to collect 1000 articles\n",
        "for page in range(100):  # pages 0 to 99 for 1000 articles (if available)\n",
        "    params = {\n",
        "        'api-key': api_key,\n",
        "        'page': page,\n",
        "        'q': 'Snow'  # Optionally, add a search term here if desired.\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error on page {page}: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "    data = response.json()\n",
        "    docs = data.get('response', {}).get('docs', [])\n",
        "\n",
        "    # If there are no more articles, exit the loop\n",
        "    if not docs:\n",
        "        print(\"No more articles found.\")\n",
        "        break\n",
        "\n",
        "    for doc in docs:\n",
        "        title = doc.get('headline', {}).get('main', '')\n",
        "        link = doc.get('web_url', '')\n",
        "        # Prefer 'abstract' if available; otherwise, use 'snippet'\n",
        "        description = doc.get('abstract', '') or doc.get('snippet', '')\n",
        "        articles.append([title, link, description])\n",
        "\n",
        "    print(f\"Fetched page {page+1}, total articles so far: {len(articles)}\")\n",
        "\n",
        "    # Pause briefly to respect rate limits (adjust as needed)\n",
        "    time.sleep(6)\n",
        "\n",
        "    # Stop if we've collected 1000 articles\n",
        "    if len(articles) >= 1000:\n",
        "        articles = articles[:1000]\n",
        "        break\n",
        "\n",
        "# Create a DataFrame with the desired columns and shape (1000 rows, 3 columns)\n",
        "df = pd.DataFrame(articles, columns=['title', 'link', 'description'])\n",
        "df.to_csv('nyt_data.csv', index=False)\n",
        "print(\"Data saved to nyt_data.csv\")\n",
        "\n",
        "# If you're running in Google Colab, this code will prompt a file download:\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('nyt_data.csv')\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab; please locate the 'nyt_data.csv' file in your working directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "xDIy1J1RGhb7",
        "outputId": "bbce92df-d8ab-4472-9374-ff61cf6385a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched page 1, total articles so far: 10\n",
            "Fetched page 2, total articles so far: 20\n",
            "Fetched page 3, total articles so far: 30\n",
            "Fetched page 4, total articles so far: 40\n",
            "Fetched page 5, total articles so far: 50\n",
            "Error on page 5: 429\n",
            "Data saved to nyt_data.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e74253cc-8db0-4977-930a-c7d23c72800b\", \"nyt_data.csv\", 13755)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmyKBjWtG_i0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}